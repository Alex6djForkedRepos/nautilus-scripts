#!/usr/bin/env bash

# Source the script '_common-functions.sh'.
SCRIPT_DIR=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
ROOT_DIR=$(grep --only-matching "^.*scripts[^/]*" <<<"$SCRIPT_DIR")
source "$ROOT_DIR/_common-functions.sh"

_main() {
    local input_files=""
    local output_dir=""

    # Execute initial checks.
    _dependencies_check_commands "curl"
    _display_wait_box "2"
    input_files=$(_get_files "par_type=file")

    # Execute the function '_main_task' for each file in parallel.
    _run_task_parallel "$(_prepare_input "$input_files")" "$output_dir"

    local std_output=""
    std_output=$(_storage_text_read_all)
    std_output=$(_text_sort "$std_output")

    _display_list_box "$std_output" "par_columns='--column:HTTP code,--column:URL'; par_item_name=URLs; par_action=open_url; par_checkbox=true"
}

_main_task() {
    local input_file=$1
    local output_dir=$2
    local std_output=""
    local timeout_value="5"
    local status=""

    # Run the main process.
    std_output=$(LC_ALL=C curl --max-time "$timeout_value" --connect-timeout 3 -o /dev/null -s -w "%{http_code}" "$input_file" 2>/dev/null)

    if [[ -z "$std_output" ]]; then
        return
    fi

    case "$std_output" in
    1* | 2* | 3*)
        status="ðŸŸ¢"
        ;;
    4* | 5*)
        status="ðŸŸ "
        ;;
    0*)
        status="ðŸ”´"
        ;;
    *)
        status="âšª"
        ;;
    esac

    _storage_text_write_ln "$status $std_output$FIELD_SEPARATOR$input_file"
}

_extract_urls() {
    # This function extracts unique URLs or domain-like strings from input
    # text. It matches three main types:
    #  - URLs with IP addresses (with optional port and path)
    #  - URLs with domain names (with optional port, path, or subdomains)
    #  - Plain domain names, with or without "www."
    #
    # Example matches:
    #  - http://192.168.0.1:8080/api
    #  - https://example.com/path
    #  - www.google.com
    #  - example.org
    #
    # The function sorts and deduplicates the matches before printing.

    local data=${1,,}
    local re_ip="https?://(?:[0-9]{1,3}\.){3}[0-9]{1,3}"
    local re_port_path="(?::[0-9]+)?(?:/[^[:space:]'\"\)\]\>]*)?"
    local re_domain="https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
    local re_plain="(?:www\.)?[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"

    if [[ -z "$data" ]]; then
        return
    fi

    data=$(grep --only-matching --perl-regexp \
        "($re_ip$re_port_path|$re_domain$re_port_path|$re_plain)" <<<"$data")
    data=$(sort --unique <<<"$data")

    printf "%s" "$data"
}

_prepare_input() {
    local input_files=$1
    local file_data=""
    local urls=""

    # shellcheck disable=SC2086
    file_data=$(cat -- $input_files 2>/dev/null)
    urls=$(_extract_urls "$file_data")

    if [[ -z "$urls" ]]; then
        _display_error_box "There are no valid URLs in the selected file(s)!"
        _exit_script
    fi

    _convert_text_to_delimited_string "$urls"
}

_main "$@"
